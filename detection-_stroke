# -*- coding: utf-8 -*-
"""copy-of-copy-of-copy-of-detection-_stroke.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/MostafaAshrafElshemy/1ed8a3dd361e2a2066e99e2663d471b9/copy-of-copy-of-copy-of-detection-_stroke.ipynb
"""



from google.colab import drive
drive.mount('/content/drive')

"""About Dataset
Similar Datasets
[HIGHLIGHTED] CERN Electron Collision Data ☄️LINK

Hepatitis C Dataset: LINK

Body Fat Prediction Dataset: LINK

Cirrhosis Prediction Dataset: LINK

Heart Failure Prediction Dataset: LINK

Stellar Classification Dataset - SDSS17: LINK

Wind Speed Prediction Dataset: LINK

Spanish Wine Quality Dataset: LINK

Context
According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.

This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.

Attribute Information
1) id: unique identifier

2) gender: "Male", "Female" or "Other"

3) age: age of the patient

4) hypertension: 0 if the patient doesn't have 

hypertension, 1 if the patient has hypertension

5) heart_disease: 0 if the patient doesn't have any heart
 diseases, 1 if the patient has a heart disease

6) ever_married: "No" or "Yes"

7) work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"

8) Residence_type: "Rural" or "Urban"

9) avg_glucose_level: average glucose level in blood

10) bmi: body mass index

11) smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"*

12) stroke: 1 if the patient had a stroke or 0 if not
*Note: "Unknown" in smoking_status means that the information is unavailable for this patient

Acknowledgements
(Confidential Source) - Use only for educational purposes
If you use this dataset in your research, please credit the author.
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, roc_curve
import os
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report

import warnings
warnings.filterwarnings("ignore")

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/stroke prediction/healthcare-dataset-stroke-data.csv",encoding="UTF-8")
data.head()

# information about data frame
data.info()

"""## Missing Value Analysis"""

## Missing Value Analysis
data.isnull().sum()

plt.figure(figsize=(20,8))
sns.heatmap(data.isnull())

"""#The following code shows how to fill the NaN values in 
#the rating column with the mean value of the rating

"""

data['bmi'].fillna(data['bmi'].mean(), inplace = True)

data.isnull().sum()

# information about data frame
data.info()

# describe basic statistics of data
data.describe()

# describe basic statistics of data T
data.describe().T

"""I dropped, by removing the id column, because we moved it to train it, because it is irrelevant
At all stroke determination
"""

data=data.drop(["id"], axis=1)
data.isna().sum()

plt.figure(figsize=(20,8))
sns.heatmap(data.isnull())

"""## Data Analysis"""

#Determine the number of heart patients present in the data

class_counts =plt.pie(data['heart_disease'].value_counts(),autopct='%1.2f%%',
                      labels=data['heart_disease'].value_counts().index)
plt.ylabel('Presence of Heart Disease')
plt.show()

#A drawing showing the number of smokers,ex-smokers,non-smokers and unknown persons
smoking_counts = data['smoking_status'].value_counts().sort_values(ascending=True)
sns.set_style("whitegrid")
colors = ['red', 'black', 'blue', 'yellow']

# Create horizontal bar chart of airline counts
smoking_counts.plot(kind='barh', color=colors)
plt.title("Number of Patients")
plt.xlabel("Count")
plt.ylabel("Smoking Status")
plt.show()

plt.hist(data['avg_glucose_level'], bins=10)

plt.title('Patients on varying Glucose Levels')
plt.xlabel('Number of Patients')
plt.ylabel('Glucose Levels')

# Show the plot
plt.show()

sns.countplot(data = data, x = 'stroke')
plt.title("Stroke Occurance")
plt.show()

sns.lineplot(data = data, x = 'smoking_status', y = 'age', hue = 'stroke')
plt.title("Effect of Smoking and Age on Stroke")
plt.show()

#Visualising the relationship between different columns of the data
sns.pairplot(data, height = 2)
plt.show()

#Counting the occurance of stroke in the data
sns.countplot(data = data, x = 'ever_married', hue = 'stroke')
plt.title("Effect of Smoking and Age on Stroke")
plt.show()

fig, ax = plt.subplots(figsize = (20,10))
sns.boxplot(data = data, x = 'smoking_status', y = 'bmi', hue = 'stroke',
fliersize = 3)
plt.title("Effect of Smoking and BMI on Stroke", fontdict = {'size' : 25})
plt.xlabel("Smokers or Non-Smokers", fontdict = {'size' : 16})
plt.ylabel("Body Mass Index", fontdict = {'size' : 16})
plt.show()

# Plotting a heatmap/correlation plot to see how different values arerelated to each other
plt.figure(figsize=(15,12))
sns.heatmap(data.corr(),annot=True,linewidths=2, center = True)
plt.show()

#Making sure that no NA values are left in the data
data.isna().sum()

data.head()

data.info()

"""## processing data"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
data.gender=le.fit_transform(data.gender)
data.ever_married=le.fit_transform(data.ever_married)
data.work_type=le.fit_transform(data.work_type)
data.Residence_type=le.fit_transform(data.Residence_type)
data.smoking_status=le.fit_transform(data.smoking_status)

data.shape

# SETTING TARGET VARIABLES

y = data['stroke']

# Extract the input features
x = data.drop(['stroke'], axis=1)

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size= 0.25 , random_state= 42)
print(x_train.shape , x_test.shape)

# Apply SMOTE to oversample the minority class
from imblearn.over_sampling import SMOTE



# Generate and plot a synthetic imbalanced classification dataset
from collections import Counter
from sklearn.datasets import make_classification
from matplotlib import pyplot
from numpy import where
# define dataset
x, y = make_classification(n_samples=5110, n_features=2, n_redundant=0,
                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)
# summarize class distribution
counter = Counter(y)
print(counter)
# scatter plot of examples by class label
for label, _ in counter.items():
 row_ix = where(y == label)[0]
 pyplot.scatter(x[row_ix, 0], x[row_ix, 1], label=str(label))
pyplot.legend()
pyplot.show()

oversample = SMOTE()
x_train, y_train = oversample.fit_resample(x_train, y_train)

oversample = SMOTE()
x_test, y_test = oversample.fit_resample(x_test, y_test)

# summarize the new class distribution
counter = Counter(y_train)
print(counter)

x_train.shape , y_train.shape

x_test.shape , y_test.shape

from sklearn.linear_model import LogisticRegression

lg = LogisticRegression(max_iter = 5110)
lg.fit(x_train, y_train)

lg.score(x_train , y_train)

lg.score(x_test ,y_test)

lg.intercept_

lg.coef_

y_predict = lg.predict(x_test)
df = pd.DataFrame({"Y_test": y_test , "Y_predict" : y_predict})
df

plt.figure(figsize=(15,8))
sns.countplot(df[:5110])

print(classification_report(y_test, y_predict))

# import linrary
from xgboost import XGBClassifier

xgb_model = XGBClassifier().fit(x_train, y_train)

# predict
xgb_y_predict = xgb_model.predict(x_test)

# accuracy score
xgb_score = accuracy_score(y_predict, y_test)

print('Accuracy score is:', xgb_score)

print(classification_report(y_test, y_predict))

import imblearn

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42, replacement=True)
# fit predictor and target variable
x_rus, y_rus = rus.fit_resample(x, y)

print('original dataset shape:', Counter(y))
print('Resample dataset shape', Counter(y_rus))

#KNN model

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(x_rus , y_rus)

knn.score(x_rus , y_rus )

knn.score(x_test , y_test)

y_predict = knn.predict(x_rus)
df = pd.DataFrame({"Y_test" : y_rus, "Y_predict" : y_predict})
df.head(10)

print(classification_report(y_rus, y_predict))







"""#Create ConfusionMatrixDisplay¶
#With the fitted model, we compute the predictions of the model on the test dataset. These predictions are used to compute the confustion matrix which is plotted with the ConfusionMatrixDisplay
"""

from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))
clf.fit(x_rus, y_rus)

y_pred = clf.predict(x_rus)
cm = confusion_matrix(y_rus,y_pred)

cm_display = ConfusionMatrixDisplay(cm).plot()
plt.show()

"""#classification_report"""

from sklearn.metrics import classification_report
print(classification_report(y_rus, y_pred))

"""#Create PrecisionRecallDisplay¶
Similarly, the precision recall curve can be plotted using y_score from the prevision sections.
"""

from sklearn.metrics import roc_curve
from sklearn.metrics import RocCurveDisplay

y_score = clf.decision_function(x_rus)

fpr, tpr, _ = roc_curve(y_rus, y_score, pos_label=clf.classes_[1])
roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
plt.show()



"""#Create RocCurveDisplay
The roc curve requires either the probabilities or the non-thresholded decision values from the estimator. Since the logistic regression provides a decision function, we will use it to plot the roc curve:
"""

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import PrecisionRecallDisplay

prec, recall, _ = precision_recall_curve(y_rus, y_score, pos_label=clf.classes_[1])
pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()
plt.show()

"""#Combining the display objects into a single plot¶
The display objects store the computed values that were passed as arguments. This allows for the visualizations to be easliy combined using matplotlib’s API. In the following example, we place the displays next to each other in a row.
"""

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))

roc_display.plot(ax=ax1)
pr_display.plot(ax=ax2)
plt.show()

"""#cluster  
# KMeans
"""

from sklearn.cluster import KMeans
km = KMeans(n_clusters= 5)
y_pred = km.fit_predict(data)

y_pred

centroids = km.cluster_centers_
centroids

data['y'] = y_pred
data

data['y'].value_counts()



#data.plot(kind = "scatter" , data = "lat" , data="long")

#plt.scatter( df['lat'] ,df["long"] , c = df['y'])
