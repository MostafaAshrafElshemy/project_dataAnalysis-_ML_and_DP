# -*- coding: utf-8 -*-
"""Bank Loan Status Data_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E0KXY4ofZqBPmTWC0Piu6YQnRrdHJbdG
"""



from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bank Loan Status Dataset/credit_train.csv" , encoding='utf-8')
data.head()

data.info()

data.describe()

data.describe().T

data.columns

data.shape

#A function to calculate and print out the missing value and it percentage

def calculate_null_values(dataframe):
    d_frame = dataframe
    # get the sum of the null value of  each column 
    d_frame_null_values = pd.DataFrame(dataframe.isna().sum())
    # reset the dataframe index
    d_frame_null_values.reset_index(inplace=True)
    # add colume header to the dataframe
    d_frame_null_values.columns = ['Field_names', 'Null_value']
    #calculate the percentage of null or missing values 
    d_frame_null_value_percentage = dataframe.isnull().sum() / len(dataframe) * 100
    d_frame_null_value_percentage = pd.DataFrame(d_frame_null_value_percentage)
    d_frame_null_value_percentage.reset_index(inplace=True)
    d_frame_null_value_percentage = pd.DataFrame(d_frame_null_value_percentage)
    d_frame_null_values['Null_values_percentage'] = d_frame_null_value_percentage[0]
    return d_frame_null_values

calculate_null_values(data)

plt.Figure(figsize=(25,12))
sns.heatmap(data.isnull())

#Data preprocessing
data_train = data.drop(labels=['Loan ID', 'Customer ID'], axis=1)

data.shape

data_train.shape

#Dealing with missing values
# about 50 % of it is missing 
data_train.drop(columns = 'Months since last delinquent', axis=1, inplace=True)
calculate_null_values(data_train)

data_train[data_train['Years of Credit History'].isnull() == True]

## We note that the last 514 values are misiing values 
data_train.drop(data.tail(514).index, inplace=True)
# drop last 514 rows
calculate_null_values(data_train)

for i in data_train['Maximum Open Credit'][data_train['Maximum Open Credit'].isnull() == True].index:
    data_train.drop(labels=i, inplace=True)

for i in data_train['Tax Liens'][data_train['Tax Liens'].isnull() == True].index:
    data_train.drop(labels=i, inplace=True)

for i in data_train['Bankruptcies'][data_train['Bankruptcies'].isnull() == True].index:
    data_train.drop(labels=i, inplace=True)

calculate_null_values(data_train)

data.shape

data_train.shape

sns.boxenplot(data = data_train , x = "Credit Score" ,  )

sns.displot(data = data_train , x = "Credit Score" ,  kind="kde",)

fill_list = data_train['Credit Score'].dropna()
data_train['Credit Score'] = data['Credit Score'].fillna(pd.Series(np.random.choice(fill_list , size = len(data.index))))

data_train.dropna(axis = 0, subset = ['Credit Score'], inplace = True)

sns.displot(data = data_train , x = "Credit Score" ,  kind="kde",)

calculate_null_values(data_train)

data_train.shape

sns.kdeplot(data_train['Annual Income'],
                color="Red", shade = True)

sns.boxplot(data_train['Annual Income'],color="Red")

sns.catplot(data_train['Annual Income'],color="Red")

fill_list = data_train['Annual Income'].dropna()
data_train['Annual Income'] = data_train['Annual Income'].fillna(pd.Series(np.random.choice(fill_list , size = len(data_train.index))))

data_train.dropna(axis = 0, subset = ['Annual Income'], inplace = True)

sns.kdeplot(data_train['Annual Income'],
                color="Red", shade = True)

data_train.info()

plt.figure(figsize=(16,8))

sns.boxplot(data_train['Years of Credit History'])

plt.figure(figsize=(16,8))

sns.catplot(data_train['Years in current job'])

data_train['Years in current job'].fillna('10+ years', inplace=True)
# fill with '10+ years'.

calculate_null_values(data_train)

data_train.shape

plt.figure(figsize=(16,10))
sns.heatmap(data.isnull())

plt.figure(figsize=(16,10))
sns.heatmap(data_train.isnull())

###Drop the duplicated value and edit some values

data_train.duplicated().sum()

data_train.drop_duplicates(inplace=True)

data_train.shape

def v_counts(dataframe):
    for i in dataframe :
        print(dataframe[i].value_counts())
        print("_____________________________________________________________________________")

v_counts(data_train)

data_train.Purpose = data_train.Purpose.str.replace('other','Other')

from sklearn.preprocessing import LabelEncoder

l_encoder = LabelEncoder()

data_train['Loan Status'] = l_encoder.fit_transform(data_train['Loan Status'])
data_train['Term'] = l_encoder.fit_transform(data_train['Term'])
data_train['Years in current job'] = l_encoder.fit_transform(data_train['Years in current job'])
data_train['Home Ownership'] = l_encoder.fit_transform(data_train['Home Ownership'])
data_train['Purpose'] = l_encoder.fit_transform(data_train['Purpose'])

v_counts(data_train)

#Split the data

from sklearn.model_selection import train_test_split

x= data_train.drop(['Loan Status' ] , axis=1).values
y = data_train['Loan Status'].values

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size= 0.25 , random_state= 42)

#from pandas.core.common import random_state
#x_train ,x_test , y_train , y_test = train_test_split(y,x,test_size=0.25 ,random_state=42)

print(x_train.shape , x_test.shape)

#Data Scalling
from sklearn.preprocessing import StandardScaler

scalar = StandardScaler()

x_train=scalar.fit_transform(x_train)

#x_train = scalar.fit_transform(x_train)

x_test = scalar.fit_transform(x_test)

x_train.shape

from sklearn.linear_model import LogisticRegression

lg = LogisticRegression(max_iter = 500)

lg.fit(x_train , y_train)

lg.score(x_train , y_train)

lg.score(x_test ,y_test)

lg.intercept_

lg.coef_

y_predict = lg.predict(x_test)
df = pd.DataFrame({"Y_test": y_test , "Y_predict" : y_predict})
df

plt.figure(figsize=(15,8))
plt.plot(df[:100])

###Test the model

pass1 = [0,20,1,0,1,254,3,15,4,2,1,4 ,2,4,5]
pass2 = [1,40,1,1,0,300,2,10,2,4,0,2,1,3,1]
lg.predict([pass1])
lg.predict([pass2])

#KNN model

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(x_train , y_train)

knn.score(x_train , y_train )

knn.score(x_test , y_test)

y_predict = knn.predict(x_test)
df = pd.DataFrame({"Y_test" : y_test , "Y_predict" : y_predict})
df.head(10)

knn.predict([pass1])

knn.predict([pass2])

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()

gnb.fit(x_train , y_train)

gnb.score(x_train , y_train)

gnb.score(x_test , y_test)

y_predict= gnb.predict(x_test)
gnb = pd.DataFrame({"Y_test": y_test , "Y_predict" : y_predict})
gnb.head(10)

pass1 = [0,20,1,0,1,254,3,15,4,2,1,4 ,2,4,5]
pass2 = [1,40,1,1,0,300,2,10,2,4,0,2,1,3,1]
lg.predict([pass1])
lg.predict([pass2])



## SVC
from sklearn.svm import SVC

ssvc = SVC( C=2, kernel='rbf'  , probability=True)

ssvc.fit(x_train , y_train)

ssvc.score(x_train, y_train)

ssvc.score(x_test , y_test)

y_predict = ssvc.predict(x_text)
df =pd.DataFrame({"Y_test" : y_test , "Y_predict" : y_predict})
df.head(10)



#Under_Sampling

from collections import Counter
from sklearn.datasets import make_classification
from sklearn.cluster import MiniBatchKMeans
from imblearn.under_sampling import ClusterCentroids

from imblearn.under_sampling import ClusterCentroids
undersampler = ClusterCentroids()

X_smote, y_smote = undersampler.fit_resample(x_train, y_train)

y_smote.value_counts()

x_smote

#Over_Sampling

from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import RandomOverSampler

x_train , y_train = make_classification(n_samples=500, weights=[0.99], flip_y=0)

# summarize class distribution
print(Counter(y_train))

# define oversampling strategy
oversample = RandomOverSampler(sampling_strategy='minority')

x_over, y_over = oversample.fit_resample(x_train, y_train)

# summarize class distribution
print(Counter(y_over)

#Combine Oversampling and Undersampling for Imbalanced Classification

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from imblearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from imblearn.combine import SMOTEENN

X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,
                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)

# define model
model = DecisionTreeClassifier()

# define resampling
resample = SMOTEENN()

# define pipeline
pipeline = Pipeline(steps=[('r', resample), ('m', model)])

# define evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

# evaluate model
scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)

# summarize performance
print('Mean ROC AUC: %.3f' % mean(scores))



#EnSemble

from sklearn.metrics import mean_squared_error
 
# importing machine learning models for prediction
import xgboost as xgb
 
# importing bagging module
from sklearn.ensemble import BaggingRegressor

# loading train data set in dataframe from train_data.csv file
df = pd.read_csv("train_data.csv")

# loading train data set in dataframe from train_data.csv file
data_test = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bank Loan Status Dataset/credit_test.csv")

# getting target data from the dataframe
target = data_test["target"]

# getting train data from the dataframe
train = df.drop("target")

# Splitting between train data into training and validation dataset
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.20,random_state=42)

# initializing the bagging model using XGboost as base model with default parameters
model = BaggingRegressor(base_estimator=xgb.XGBRegressor())

# training model
model.fit(X_train, y_train)

# predicting the output on the test dataset
pred = model.predict(X_test)

# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))

# initializing the boosting module with default parameters
model = GradientBoostingRegressor()
 
# training the model on the train dataset
model.fit(X_train, y_train)
 
# predicting the output on the test dataset
pred_final = model.predict(X_test)
 
# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))



